{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For forward propagation\n",
    "# Requirements - \n",
    "# g(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.matrix([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a.getT().getA()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09003057,  0.24472847,  0.66524096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.09003057],\n",
       "        [ 0.24472847],\n",
       "        [ 0.66524096]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    numberOfLayers = 0 #Input neurons are counted as one layer\n",
    "    lengthOfFirstLayer = 0 #Including bias\n",
    "    lengthOfLastLayer = 0 #Including bias\n",
    "    learningRate = 0.1\n",
    "    activationVals = {}\n",
    "    thetaVals = {}\n",
    "    delVals = {}\n",
    "    \n",
    "    def __init__(self,numberOfLayers, lengthOfFirstLayer, lengthOfLastLayer):\n",
    "        self.numberOfLayers = numberOfLayers\n",
    "        self.lengthOfFirstLayer = lengthOfFirstLayer\n",
    "        self.lengthOfLastLayer = lengthOfLastLayer\n",
    "        self.learningRate = 0.1\n",
    "        self.initializeTheta()\n",
    "        self.initializeActivation()\n",
    "\n",
    "    def initializeTheta(self): #Should put random numbers between -1 and 1\n",
    "        for i in range(1,self.numberOfLayers-1):\n",
    "            self.thetaVals[i] = np.matrix(np.random.randn(self.lengthOfFirstLayer, self.lengthOfFirstLayer))\n",
    "        self.thetaVals[self.numberOfLayers-1] = np.matrix(np.random.randn(self.lengthOfLastLayer, self.lengthOfFirstLayer))\n",
    "\n",
    "    def initializeActivation(self):\n",
    "        for i in range(1,self.numberOfLayers):\n",
    "            self.activationVals[i] = np.matrix(np.zeros((self.lengthOfFirstLayer,1)))\n",
    "        self.activationVals[self.numberOfLayers] = np.matrix(np.zeros((self.lengthOfLastLayer,1)))\n",
    "    \n",
    "    def g(self, product):\n",
    "        result = 1/(1+math.exp(-product))\n",
    "        return result\n",
    "        \n",
    "    def printVals(self):\n",
    "        print self.numberOfLayers, self.lengthOfFirstLayer, self.lengthOfLastLayer\n",
    "        print self.thetaVals\n",
    "        print self.activationVals\n",
    "    \n",
    "    #Input is a matrix of single column\n",
    "    def softmax(self,x):\n",
    "        assert(x.shape[1] == 1)\n",
    "        returnVal = x.getT().getA()[0] #Getting the array\n",
    "        returnVal = np.exp(returnVal) / np.sum(np.exp(returnVal), axis=0)\n",
    "        returnVal = np.matrix(returnVal).getT()\n",
    "        return returnVal\n",
    "    \n",
    "    # Mostly working\n",
    "    # Can use vectorize - came to know about it later\n",
    "    def forwardPropagation(self, features): #Features includes no bias\n",
    "        assert(features.shape[1]==1)\n",
    "        self.activationVals[1] = np.append(np.zeros((1,1)),features,axis=0) #Added bias\n",
    "        self.activationVals[1][0] = 1\n",
    "        assert(self.activationVals[1].shape[0] == features.shape[0] + 1)\n",
    "        assert(self.activationVals[1].shape[1] == 1)\n",
    "        \n",
    "        for layer in range(2, self.numberOfLayers + 1):\n",
    "            self.activationVals[layer] = self.thetaVals[layer-1] * self.activationVals[layer-1]\n",
    "            self.activationVals[layer][0] = 1\n",
    "            numberOfNeurons = self.lengthOfFirstLayer\n",
    "            if layer == self.numberOfLayers:\n",
    "                numberOfNeurons = self.lengthOfLastLayer\n",
    "            for neuronNumber in range(1, numberOfNeurons):\n",
    "                self.activationVals[layer][neuronNumber] = self.g(self.activationVals[layer][neuronNumber])\n",
    "        \n",
    "        return self.softmax(self.activationVals[self.numberOfLayers][1:,0])\n",
    "    \n",
    "    #Ran forwardProp algo separately before\n",
    "    def backPropagation(self, expectedResult):\n",
    "        expectedResultWithBias = np.append(np.zeros((1,1)),expectedResult,axis=0)\n",
    "        expectedResultWithBias[0] = 1\n",
    "        resultGot = self.activationVals[self.numberOfLayers]\n",
    "        assert(expectedResultWithBias.shape == resultGot.shape)\n",
    "        self.delVals[self.numberOfLayers] = self.activationVals[self.numberOfLayers] - expectedResultWithBias\n",
    "        assert(self.delVals[self.numberOfLayers][0] == 0)\n",
    "        for layer in list(reversed(range(2,self.numberOfLayers))):\n",
    "            derivativeTerm = np.multiply(self.activationVals[layer], 1 - self.activationVals[layer])\n",
    "            nonDerivativeTerm = self.thetaVals[layer].getT()*self.delVals[layer+1]\n",
    "            self.delVals[layer] = np.multiply(nonDerivativeTerm, derivativeTerm)\n",
    "            assert(self.delVals[layer][0] == 0)\n",
    "        \n",
    "    def changeTheta(self):\n",
    "        for layer in range(1,self.numberOfLayers-1): #If 3 layers, we have to do only 1\n",
    "            for i in range(self.lengthOfFirstLayer):\n",
    "                for j in range(self.lengthOfFirstLayer):\n",
    "                    self.thetaVals[layer][i,j] = self.thetaVals[layer][i,j] - \\\n",
    "                                                    self.learningRate*self.activationVals[layer][j]*self.delVals[layer+1][i]\n",
    "        layer = self.numberOfLayers-1\n",
    "        for i in range(self.lengthOfLastLayer):\n",
    "            for j in range(self.lengthOfFirstLayer):\n",
    "                self.thetaVals[layer][i,j] = self.thetaVals[layer][i,j] - \\\n",
    "                                                self.learningRate*self.activationVals[layer][j]*self.delVals[layer+1][i]\n",
    "                    \n",
    "        \"\"\"\n",
    "        # Internal layers\n",
    "        for layer in range(2,self.numberOfLayers):\n",
    "            for neuronNumber in range(self.lengthOfFirstLayer+1):\n",
    "                self.activationVals[layer][neuronNumber] = g(self.thetaVals[layer-1][neuronNumber,:]*self.activationVals[layer-1])\n",
    "            self.activationVals[layer][0] = 1\n",
    "        \n",
    "        #Outer layer\n",
    "        for neuronNumber in range(self.lengthOfLastLayer+1):\n",
    "            self.activationVals[self.numberOfLayers][neuronNumber] = g(self.thetaVals[self.numberOfLayers-1][neuronNumber,:]*\n",
    "                                                                      self.activationVals[self.numberOfLayers-1])\n",
    "        return self.activationVals[self.numberOfLayers][1:,0]\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = neuralNetwork(3,4,3)\n",
    "features = np.matrix([1,1,1]).getT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.20459328,  0.43171068, -0.58994224, -1.41185569],\n",
       "         [-0.78549401,  0.05867667,  0.24993534, -0.49639143],\n",
       "         [ 0.01060462, -0.29069559,  0.01247908, -0.03742375],\n",
       "         [ 0.93993189,  0.61359118,  0.80397639, -0.28773808]]),\n",
       " 2: matrix([[ 1.16166168, -1.57202084, -0.50642715,  1.64975053],\n",
       "         [ 0.06443522, -0.33971898, -0.09391497, -0.18703471],\n",
       "         [ 0.45427049, -0.04641532,  1.22194549, -0.49960743]])}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.20459328,  0.43171068, -0.58994224, -1.41185569],\n",
       "         [-0.78869092,  0.05547976,  0.24673843, -0.49958834],\n",
       "         [-0.00937113, -0.31067135, -0.00749668, -0.05739951],\n",
       "         [ 0.9420064 ,  0.6156657 ,  0.80605091, -0.28566357]]),\n",
       " 2: matrix([[ 1.16166168, -1.57202084, -0.50642715,  1.64975053],\n",
       "         [ 0.12027469, -0.3244062 , -0.07022078, -0.13745321],\n",
       "         [ 0.39163922, -0.0635906 ,  1.19536936, -0.55521957]])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.20459328,  0.43171068, -0.58994224, -1.41185569],\n",
       "         [-0.79134769,  0.05282298,  0.24408165, -0.50224512],\n",
       "         [-0.02726847, -0.32856869, -0.02539401, -0.07529685],\n",
       "         [ 0.94452684,  0.61818614,  0.80857135, -0.28314313]]),\n",
       " 2: matrix([[ 1.16166168, -1.57202084, -0.50642715,  1.64975053],\n",
       "         [ 0.17323274, -0.31001799, -0.04877614, -0.09038662],\n",
       "         [ 0.33261521, -0.07962688,  1.17146839, -0.60767729]])}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.20459328,  0.43171068, -0.58994224, -1.41185569],\n",
       "         [-0.79354308,  0.0506276 ,  0.24188627, -0.5044405 ],\n",
       "         [-0.04331273, -0.34461295, -0.04143828, -0.09134111],\n",
       "         [ 0.94739556,  0.62105485,  0.81144006, -0.28027441]]),\n",
       " 2: matrix([[ 1.16166168, -1.57202084, -0.50642715,  1.64975053],\n",
       "         [ 0.22347481, -0.29647308, -0.02929177, -0.04568398],\n",
       "         [ 0.27701932, -0.09461515,  1.14990776, -0.65714347]])}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.backPropagation(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.changeTheta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 1.25442967,  1.05507052, -0.19241158,  0.45980842],\n",
       "         [ 1.34737066, -2.75135218, -0.3032265 ,  0.06879914],\n",
       "         [ 1.04687633,  1.22935677, -0.85185639, -0.38457311],\n",
       "         [ 0.67571302, -0.20956889,  1.14300245, -0.20112442]]),\n",
       " 2: matrix([[ 2.35215101, -0.3904102 , -1.02787376,  0.75406666],\n",
       "         [ 0.0632558 ,  1.30607959,  1.03671232,  0.59822863],\n",
       "         [ 0.77317763, -0.84469085, -0.3308197 ,  0.52432102]])}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
