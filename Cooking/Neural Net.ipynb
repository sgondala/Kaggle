{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For forward propagation\n",
    "# Requirements - \n",
    "# g(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.matrix([[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1],\n",
       "        [2],\n",
       "        [3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a.getT().getA()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09003057,  0.24472847,  0.66524096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.09003057],\n",
       "        [ 0.24472847],\n",
       "        [ 0.66524096]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class neuralNetwork:\n",
    "    numberOfLayers = 0 #Input neurons are counted as one layer\n",
    "    lengthOfFirstLayer = 0 #Including bias\n",
    "    lengthOfLastLayer = 0 #Including bias\n",
    "    learningRate = 0.1\n",
    "    activationVals = {}\n",
    "    thetaVals = {}\n",
    "    delVals = {}\n",
    "    \n",
    "    def __init__(self,numberOfLayers, lengthOfFirstLayer, lengthOfLastLayer):\n",
    "        self.numberOfLayers = numberOfLayers\n",
    "        self.lengthOfFirstLayer = lengthOfFirstLayer\n",
    "        self.lengthOfLastLayer = lengthOfLastLayer\n",
    "        self.learningRate = 0.1\n",
    "        self.initializeTheta()\n",
    "        self.initializeActivation()\n",
    "\n",
    "    def initializeTheta(self): #Should put random numbers between -1 and 1\n",
    "        for i in range(1,self.numberOfLayers-1):\n",
    "            self.thetaVals[i] = np.matrix(np.random.randn(self.lengthOfFirstLayer, self.lengthOfFirstLayer))\n",
    "        self.thetaVals[self.numberOfLayers-1] = np.matrix(np.random.randn(self.lengthOfLastLayer, self.lengthOfFirstLayer))\n",
    "\n",
    "    def initializeActivation(self):\n",
    "        for i in range(1,self.numberOfLayers):\n",
    "            self.activationVals[i] = np.matrix(np.zeros((self.lengthOfFirstLayer,1)))\n",
    "        self.activationVals[self.numberOfLayers] = np.matrix(np.zeros((self.lengthOfLastLayer,1)))\n",
    "    \n",
    "    def g(self, product):\n",
    "        result = 1/(1+math.exp(-product))\n",
    "        return result\n",
    "        \n",
    "    def printVals(self):\n",
    "        print self.numberOfLayers, self.lengthOfFirstLayer, self.lengthOfLastLayer\n",
    "        print self.thetaVals\n",
    "        print self.activationVals\n",
    "    \n",
    "    #Input is a matrix of single column\n",
    "    def softmax(self,x):\n",
    "        assert(x.shape[1] == 1)\n",
    "        returnVal = x.getT().getA()[0] #Getting the array\n",
    "        returnVal = np.exp(returnVal) / np.sum(np.exp(returnVal), axis=0)\n",
    "        returnVal = np.matrix(returnVal).getT()\n",
    "        return returnVal\n",
    "    \n",
    "    # Mostly working\n",
    "    # Can use vectorize - came to know about it later\n",
    "    def forwardPropagation(self, features): #Features includes no bias\n",
    "        assert(features.shape[1]==1)\n",
    "        self.activationVals[1] = np.append(np.zeros((1,1)),features,axis=0) #Added bias\n",
    "        self.activationVals[1][0] = 1\n",
    "        assert(self.activationVals[1].shape[0] == features.shape[0] + 1)\n",
    "        assert(self.activationVals[1].shape[1] == 1)\n",
    "        \n",
    "        for layer in range(2, self.numberOfLayers + 1):\n",
    "            print layer\n",
    "            self.activationVals[layer] = self.thetaVals[layer-1] * self.activationVals[layer-1]\n",
    "            self.activationVals[layer][0] = 1\n",
    "            numberOfNeurons = self.lengthOfFirstLayer\n",
    "            if layer == self.numberOfLayers:\n",
    "                numberOfNeurons = self.lengthOfLastLayer\n",
    "            for neuronNumber in range(1, numberOfNeurons):\n",
    "                self.activationVals[layer][neuronNumber] = self.g(self.activationVals[layer][neuronNumber])\n",
    "        \n",
    "        return self.softmax(self.activationVals[self.numberOfLayers][1:,0])\n",
    "    \n",
    "    #Ran forwardProp algo separately before\n",
    "    def backPropagation(self, expectedResult):\n",
    "        expectedResultWithBias = np.append(np.zeros((1,1)),expectedResult,axis=0)\n",
    "        expectedResultWithBias[0] = 1\n",
    "        resultGot = self.activationVals[self.numberOfLayers]\n",
    "        assert(expectedResultWithBias.shape == resultGot.shape)\n",
    "        self.delVals[self.numberOfLayers] = self.activationVals[self.numberOfLayers] - expectedResultWithBias\n",
    "        assert(self.delVals[self.numberOfLayers][0] == 0)\n",
    "        for layer in list(reversed(range(2,self.numberOfLayers))):\n",
    "            derivativeTerm = np.multiply(self.activationVals[layer], 1 - self.activationVals[layer])\n",
    "            nonDerivativeTerm = self.thetaVals[layer].getT()*self.delVals[layer+1]\n",
    "            self.delVals[layer] = np.multiply(nonDerivativeTerm, derivativeTerm)\n",
    "            assert(self.delVals[layer][0] == 0)\n",
    "        \n",
    "    def changeTheta(self):\n",
    "        for layer in range(1,self.numberOfLayers):\n",
    "            DelMatrix = self.delVals[layer+1]*self.activationVals[layer].getT()\n",
    "            self.thetaVals[layer] = self.thetaVals[layer] - self.learningRate*DelMatrix\n",
    "        \n",
    "        \"\"\"\n",
    "        for layer in range(1,self.numberOfLayers-1): #If 3 layers, we have to do only 1\n",
    "            for i in range(self.lengthOfFirstLayer):\n",
    "                for j in range(self.lengthOfFirstLayer):\n",
    "                    self.thetaVals[layer][i,j] = self.thetaVals[layer][i,j] - \\\n",
    "                                                    self.learningRate*self.activationVals[layer][j]*self.delVals[layer+1][i]\n",
    "        layer = self.numberOfLayers-1\n",
    "        for i in range(self.lengthOfLastLayer):\n",
    "            for j in range(self.lengthOfFirstLayer):\n",
    "                self.thetaVals[layer][i,j] = self.thetaVals[layer][i,j] - \\\n",
    "                                                self.learningRate*self.activationVals[layer][j]*self.delVals[layer+1][i]\n",
    "        \n",
    "        # Internal layers\n",
    "        for layer in range(2,self.numberOfLayers):\n",
    "            for neuronNumber in range(self.lengthOfFirstLayer+1):\n",
    "                self.activationVals[layer][neuronNumber] = g(self.thetaVals[layer-1][neuronNumber,:]*self.activationVals[layer-1])\n",
    "            self.activationVals[layer][0] = 1\n",
    "        \n",
    "        #Outer layer\n",
    "        for neuronNumber in range(self.lengthOfLastLayer+1):\n",
    "            self.activationVals[self.numberOfLayers][neuronNumber] = g(self.thetaVals[self.numberOfLayers-1][neuronNumber,:]*\n",
    "                                                                      self.activationVals[self.numberOfLayers-1])\n",
    "        return self.activationVals[self.numberOfLayers][1:,0]\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = neuralNetwork(3,4,3)\n",
    "features = np.matrix([1,1,1]).getT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.46296093, -0.0651621 ,  1.09753935,  0.67522809],\n",
       "         [-0.6969412 ,  0.34752842,  0.79617536, -2.91191235],\n",
       "         [-2.40267916,  0.10302967,  0.40279592, -0.6320986 ],\n",
       "         [ 0.26451742,  0.99752832, -0.53873337, -1.67936602]]),\n",
       " 2: matrix([[ 1.08481981, -0.09323942,  2.33786436, -0.31965006],\n",
       "         [ 0.0652104 ,  0.6850355 , -0.88040248, -1.0650807 ],\n",
       "         [-1.00440485,  2.49838085,  2.0856692 ,  0.14770949]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.46296093, -0.0651621 ,  1.09753935,  0.67522809],\n",
       "         [-0.70050621,  0.34396341,  0.79261035, -2.91547737],\n",
       "         [-2.41106303,  0.0946458 ,  0.39441204, -0.64048247],\n",
       "         [ 0.25151023,  0.98452112, -0.55174057, -1.69237321]]),\n",
       " 2: matrix([[ 1.08481981, -0.09323942,  2.33786436, -0.31965006],\n",
       "         [ 0.12122824,  0.68942381, -0.87626538, -1.04952628],\n",
       "         [-1.03952779,  2.4956294 ,  2.08307525,  0.13795693]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.46296093, -0.0651621 ,  1.09753935,  0.67522809],\n",
       "         [-0.70390403,  0.34056559,  0.78921253, -2.91887518],\n",
       "         [-2.4189363 ,  0.08677253,  0.38653877, -0.64835574],\n",
       "         [ 0.23944545,  0.97245634, -0.56380534, -1.70443799]]),\n",
       " 2: matrix([[ 1.08481981, -0.09323942,  2.33786436, -0.31965006],\n",
       "         [ 0.17543824,  0.69361501, -0.87238438, -1.03503296],\n",
       "         [-1.07358907,  2.49299598,  2.08063674,  0.12885047]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 0.46296093, -0.0651621 ,  1.09753935,  0.67522809],\n",
       "         [-0.7071497 ,  0.33731992,  0.78596686, -2.92212086],\n",
       "         [-2.42634849,  0.07936034,  0.37912659, -0.65576793],\n",
       "         [ 0.228231  ,  0.96124189, -0.5750198 , -1.71565244]]),\n",
       " 2: matrix([[ 1.08481981, -0.09323942,  2.33786436, -0.31965006],\n",
       "         [ 0.22791885,  0.69762193, -0.86873557, -1.02149247],\n",
       "         [-1.10665192,  2.49047162,  2.07833798,  0.12031995]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.forwardPropagation(features)\n",
    "b = np.matrix([[1],[0]])\n",
    "a.backPropagation(b)\n",
    "a.changeTheta()\n",
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.backPropagation(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.changeTheta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: matrix([[ 1.25442967,  1.05507052, -0.19241158,  0.45980842],\n",
       "         [ 1.34737066, -2.75135218, -0.3032265 ,  0.06879914],\n",
       "         [ 1.04687633,  1.22935677, -0.85185639, -0.38457311],\n",
       "         [ 0.67571302, -0.20956889,  1.14300245, -0.20112442]]),\n",
       " 2: matrix([[ 2.35215101, -0.3904102 , -1.02787376,  0.75406666],\n",
       "         [ 0.0632558 ,  1.30607959,  1.03671232,  0.59822863],\n",
       "         [ 0.77317763, -0.84469085, -0.3308197 ,  0.52432102]])}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.thetaVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
